---
title: "Segmentación de universidades en EEUU con énfasis en estudiantes de areas relacionadas a la computación"
author:
  - name: Daniel Daza Macias
    email: dadazam@unal.edu.co
  - name: Daniel Santiago Cadavid Montoya
    email: dcadavid@unal.edu.co
  - name: Jose Daniel Bustamante Arango
    email: jobustamantea@unal.edu.co
  - name: Marlon Calle Areiza
    email: mcallea@unal.edu.co
  - name: Ronald Gabriel Palencia
    email: ropalencia@unal.edu.co
format:
  html:
    code-fold: true
jupyter: python3
echo: false
theme:
          light: flatly
          dark: darkly
toc: true
---

## Introducción

El siguiente trabajo tiene como objetivo ofrecer información sobre segmentos de universidades a estudiantes que estén interesados como en bachelors degrees en computación, ingeniería, ingeniería y tecnología o matemáticas.

Para realizar la segmentación utilizamos la base de datos encontrada en el link: https://data.world/exercises/cluster-analysis-exercise-2

## Importe y análisis de datos

A continuación procedemos a hacer el cargo de datos considerando aquellos suprimidos por privacidad como NaN: 

```{python}
#| label: tbl-import-presentacion-inicial
#| tbl-cap: Datos iniciales

import numpy as np
import pandas as pd
import seaborn as sb
from tabulate import tabulate 
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, ward

#TODO: Explicar valores Nan
#TODO: Revisar el tema de la low_memory=False
datos_raw = pd.read_csv('CollegeScorecard.csv', na_values='PrivacySuppressed',low_memory=False)

datos_raw.head(5)
```

Como se puede ver en @tbl-import-presentacion-inicial la base de datos contiene 1725 columnas además de 7804 observaciones.

Para comenzar y como nuestro interés de acuerdo al objetivo es tener la mayor cantidad de datos sin imputar relacionados a nuestro objetivo procederemos a seleccionar y estudiar las columnas cuyo porcentaje de nulos sea menor al (30%)

```{python}

def verificar_variables(nombre_archivo):
  path = f'/content/gdrive/Shareddrives/TAE/{nombre_archivo}'
  variable = pd.read_csv(path)
  nombre_ingresos = list(variable['variable_name'])
  df.loc[:, nombre_ingresos].info(verbose=True, show_counts=True)

def utiles_por_porcentaje(df, columnas, porcentaje):
  new_df = df[columnas]
  total = len(new_df)
  datos = {'columna': [], 'porcentaje_datos_nulos': [], 'datos_nulos': []}
  for col in new_df.columns:
    nulos = new_df[col].isna().sum()
    datos['columna'].append(col)
    datos['porcentaje_datos_nulos'].append(nulos/total)
    datos['datos_nulos'].append(f'{nulos} de {total}')
  nulos_columnas = pd.DataFrame(datos)
  mayor_02 = nulos_columnas['porcentaje_datos_nulos'] <= porcentaje
  utiles = nulos_columnas[mayor_02].sort_values(by='porcentaje_datos_nulos')
  return utiles

utiles_por_porcentaje(df=datos_raw, columnas=datos_raw.columns, porcentaje=0.3)

```

De acuerdo a un análisis manual y dado el objetivo planteado inicialmente se concluye que de las 351 variables resultantes las más adecuadas, en un principio, para ofrecer un producto son las presentados en @tbl-col-iniciales donde para el caso de las variables terminadas en "BACHL" TODO: Como es la vuelta de estas variables

```{python}
#| label: tbl-col-iniciales
#| tbl-cap: Columnas iniciales a analizar


def_var_dict={

"CURROPER":"Esta variable explica si la universidad se encuentra operando. Tiene 2 valores (1) si la universidad está operando o (2) si no está operando",

"CONTROL":"Tipo de institución; pública(0), privada sin ánimo de lucro(1) o privada con ánimo de lucro(2)",

"DEBT_MDN":"Se trata de la deuda mediana de préstamos acumulada en la institución por todos los estudiantes prestatarios de préstamos federales que se separan (es decir, se gradúan o se retiran) en un año fiscal determinado, medida en el punto de separación (DEBT_MDN)",

"PCTFLOAN":"Proporción de estudiantes universitarios que recibieron préstamos federales en un año determinado",

"GRAD_DEBT_MDN":"Deuda para los estudiantes que completaron sus estudios universitarios",

"PCTPELL":"Esta variable es el porcentaje de estudiantes los cuales recibieron Pell Grants, el cual es una beca federal que reciben los estudiantes de ingresos bajos",

"PCIP11":"Porcentaje de titulos otorgados en el campo de la computación",

"PCIP15":"Porcentaje de titulos otorgados en el campo de la  ingeniería",

"PCIP14":"Porcentaje de titulos otorgados en el campo de la  ingeniería y tecnología",

"PCIP27":"Porcentaje de titulos en otorgados en el campo de la  matemáticas",


"CIP11BACHL":"Identifíca si se el ofrece un bacheloor degree en computación y en que modalidad",

"CIP15BACHL":"Identifíca si se el ofrece un bacheloor degree en ingeniería y en que modalidad",

"CIP14BACHL":"Identifíca si se el ofrece un bacheloor degree en ingeniería y tecnología, y en que modalidad",

"CIP27BACHL":"Identifíca si se el ofrece un bacheloor degree en matemáticas, y en que modalidad"

}



from IPython.display import Markdown
from tabulate import tabulate
#TODO: Solucionar type de datos
interes=['CURROPER', 'CONTROL', 'DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27', 'CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL', 'PCTPELL']
table = [[interes[0],datos_raw[interes[0]].dtype,def_var_dict[interes[0]]],
         [interes[1],datos_raw[interes[1]].dtype,def_var_dict[interes[1]]],
         [interes[2],datos_raw[interes[2]].dtype,def_var_dict[interes[2]]],
         [interes[3],datos_raw[interes[3]].dtype,def_var_dict[interes[3]]],
         [interes[4],datos_raw[interes[4]].dtype,def_var_dict[interes[4]]],
         [interes[5],datos_raw[interes[5]].dtype,def_var_dict[interes[5]]],
         [interes[6],datos_raw[interes[6]].dtype,def_var_dict[interes[6]]],
         [interes[7],datos_raw[interes[7]].dtype,def_var_dict[interes[7]]],
         [interes[8],datos_raw[interes[8]].dtype,def_var_dict[interes[8]]],
         [interes[9],datos_raw[interes[9]].dtype,def_var_dict[interes[9]]],
         [interes[10],datos_raw[interes[10]].dtype,def_var_dict[interes[10]]],
         [interes[11],datos_raw[interes[11]].dtype,def_var_dict[interes[11]]],
         [interes[12],datos_raw[interes[12]].dtype,def_var_dict[interes[12]]],
         [interes[13],datos_raw[interes[13]].dtype,def_var_dict[interes[13]]],
         ]
Markdown(tabulate(
  table, 
  headers=["Nombre","Tipo de dato", "Resumen"]
))
```

En cuanto al manejo de nulos resultantes como se puede ver en @tbl-col-iniciales-datos-perdidas la mayor perdida de datos se debe a las variables DEBT_MDN y GRAD_DEBT_MDN, que tienen entre un 14% y un 16% de datos nulos en comparación con los datos generales.


```{python}
#| label: tbl-col-iniciales-datos-perdidas
#| tbl-cap: Datos perdidos

columnas_categoricas=['CURROPER', 'CONTROL','CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL']
columnas_numericas=['DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27','PCTPELL']

col_interes_NaN_analisis_all=utiles_por_porcentaje(df=datos_raw, columnas=interes, porcentaje=0.3)

col_interes_NaN_analisis_interes=utiles_por_porcentaje(df=datos_raw[interes].dropna(subset=["PCTPELL"]), columnas=interes, porcentaje=0.3)

col_interes_NaN_analisis_interes

colA=list(col_interes_NaN_analisis_all["porcentaje_datos_nulos"])
colB=list(col_interes_NaN_analisis_interes["porcentaje_datos_nulos"])
table1=list()

for i in range(14):
  if interes[i] in columnas_categoricas:
    table1.append([interes[i],"Variable categorica",colA[i]])
  else:
    table1.append([interes[i],"Variable numerica",colA[i]])

"""Markdown(tabulate(
  table1, 
  headers=["Variable","Tipo de variable", "% NaN"]
))"""

utiles_por_porcentaje(datos_raw,interes,0.3)
```

Por lo que se procede con una imputación reemplazando con la mediana y la moda los valores nulos en columnas numéricas y categóricas respectivamente,  lo que nos deja con un conjunto de datos de 7804 observaciones y 14 columnas.

Además de ello se procederá a hacer un One Hot Encoding a aquellas variables numéricas.

### Análisis inicial de datos

Comenzaremos con @fig-elephant donde se observan las densidades discriminadas por tipo de universidad (CONTROL). Para la variable deuda media acumulada se visualiza que la variabilidad de la deuda para los estudiantes de universidades privadas sin animo es mas elevada, a comparación de la publicas y privadas con animo de lucro,  ademas la media de las universidades sin ánimo de lucro es mas elevada. En la variable  que representa la proporción de estudiantes universitarios que recibieron préstamos federales en un año determinado (PCTFLOAN) se que la distribución de los datos para los estidudiantes de universidades privadas, independientemente si son de ánimo de lucro o no tienen el mismo comportamiento, a comparación de los que pertecen a la institución pública. Por otro lado; para la variable que tiene el porcentaje de estudiantes que recivierón la beca PELL  tienen distribuciones similares los que pertenecen a universidades privadas y públicas sin ánimo de lucro, por otro lado; las privadas con ánimo de lucro son las que tienen en promedio mayor porcentajes de becas y también una mayor dispersión.

![Analisis de densidad](densidad1.png){#fig-elephant}

Podemos notar en la matriz de correlación @fig-matriz-corr-1 que la deuda media acumulada de todos los estudiantes (DEBT_MDN) y la deuda media acumulada de los estudiantes que terminaron la carrera (GRAD_DEBT_MDN) presenta una correlación de 0.77, por lo que procedera a sacar una de ellas (GRAD_DEBT_MDN) para eliminar redundancia del conjunto de caracteristicas con las que se estan trabajando en el proceso de clusterización. También se logra observar unas correlaciones moderadas en otros pares de variables, son embargo se tomo la decisión de dejarlar, dado que son variables que contienen información inportante en el proceso de clusterización.


```{python}
#| label: fig-matriz-corr-1
#| fig-cap: "Matriz de correlacion"

datos_raw_interes=datos_raw[interes]
columnas_categoricas=['CURROPER', 'CONTROL','CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL']
columnas_numericas=['DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27','PCTPELL']

"""
datos_interes_imputados=datos_raw_interes.copy()
datos_interes_imputados[columnas_numericas] = datos_raw_interes[columnas_numericas].fillna(datos_raw_interes[columnas_numericas].median())

for col in columnas_categoricas:
  datos_interes_imputados[col] = datos_raw_interes[col].fillna(datos_raw_interes[col].mode()[0])
"""

datos_interes_imputados = pd.read_csv('sinNulos.csv',low_memory=False)


corrMatrix=datos_interes_imputados[columnas_numericas].corr()
sb.heatmap(corrMatrix, annot=True)
#plt.rcParams["figure.figsize"] = (25,25)
#plt.show
```

Ahora se procederá a hacer un análisis de componentes principales sobre el dataset con variables categóricas encodeadas y variables numéricas normalizadas con min-max.
TODO: Hablar antes de este parrafo de como se sacó las de DEBT mayores a 120000

```{python}
#| label: tbl-princ-component-1
#| tbl-cap: Analisis de Componentes Principales



from scipy import stats
#TODO: MIRAR ESTO
data=datos_interes_imputados.copy()
"""
data["Z_CT"]=np.abs(stats.zscore(data["DEBT_MDN"]))
data=data[data["Z_CT"]<=3]

data=data.drop("Z_CT",axis=1)
"""
data=data[data["DEBT_MDN"]<120000]
#Sacando GRAD_DEBT_MDN
imputados_OneHot=data.copy()

columnas_numericas.remove("GRAD_DEBT_MDN")
imputados_OneHot = imputados_OneHot.drop("GRAD_DEBT_MDN", axis=1)


#Dummies de variables categoricas
dummy_cols = ['CIP11BACHL', 'CIP15BACHL', 'CIP14BACHL', 'CIP27BACHL', 'CONTROL']

for col in dummy_cols:
  one_hot = pd.get_dummies(imputados_OneHot[col], prefix=col)
  imputados_OneHot = imputados_OneHot.drop(col, axis=1)
  imputados_OneHot = imputados_OneHot.join(one_hot)

#Estandarización de las variables numericas
imputados_OneHot[columnas_numericas]=(imputados_OneHot[columnas_numericas] - imputados_OneHot[columnas_numericas].min())/(imputados_OneHot[columnas_numericas].max()-imputados_OneHot[columnas_numericas].min())


#Se procede a realizar el PCA

from sklearn.decomposition import PCA


pca = PCA()
pca.fit(imputados_OneHot)


table2=list()
acc_var_sum=0
contador=1
for i in pca.explained_variance_ratio_:
  if acc_var_sum<0.95:
    acc_var_sum+=i
    table2.append(["Componente "+str(contador),i,acc_var_sum])
    contador+=1
  else:
    break

Markdown(tabulate(
  table2, 
  headers=["Num. Componente","Varianza explicada", "Varianza acomulada"]
))
#dfilo = pd.DataFrame(table2, columns=["# Compenente","Varianza explicada", "Varianza acomulada"])
#table2
```

Como se puede ver en @tbl-princ-component-1 poco más del 95% de la varianza comienza a ser explicada al tener 9 de las componentes principales por lo que se considera no hay una suficiente diferencia entre uso de variables para usar PCA, por lo que se continuará con el desarrollo del modelo con las 13 columnas escogidas.

## Desarrollo del modelo

### Cantidad de clusters

Para proceder con el desarrollo del modelo se comenzará escogiendo la cantidad de clusters o grupos en los que se segmentarán los datos. Para ello se hará un análisis de Elbow Curve:

```{python}
#| label: fig-elbow-1
#| fig-cap: Analisis de Elbow curve

def elbow_curve(data, maxClusters = 15):

  # rango de valores del parámetro a optimizar (cantidad de clusters)
  maxClusters = range(1, maxClusters + 1)
  inertias = []

  # se ejecuta el modelo para el rango de clusters y se guarda la inercia
  # respectiva obtenida para cada valor
  for k in maxClusters:
    kmeanModel = KMeans(n_clusters = k)
    kmeanModel.fit(data)
    inertias.append(kmeanModel.inertia_)
  
# Grafico de los resultados obtenidos para cada valor del rango
  #print("Valores: ",inertias)
  plt.figure(figsize=(10, 8))
  plt.plot(maxClusters, inertias, 'bx-')
  plt.xlabel('k')
  plt.ylabel('Inertia')
  plt.title('The Elbow Method showing the optimal k')
  plt.show()

elbow_curve(np.array(imputados_OneHot))
```

En @fig-elbow-1 se puede notar que la pendiente de inercia de un cluster a otro comienza a ser menor desde que hay 6 cluster por lo que se concluye esta como la cantidad ideal.

Por otra parte en @fig-dendrogram-1 se puede notar la agrupación en 6 grupos a través de agrupación aglomerativa con distancia ward en un dendrograma

```{python}
#| label: fig-dendrogram-1
#| fig-cap: Analisis de Dendrograma

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot

pyplot.figure(figsize=(15, 7))  
pyplot.title("Dendrograma") 
dend = shc.dendrogram(shc.linkage(imputados_OneHot, method='ward'),truncate_mode="lastp")# Dendrograma usando ward como método de linkage.
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [35, 35], '--', c='k')
ax.text(bounds[1], 35, ' Seis grupos', va='center', fontdict={'size': 15})
plt.xlabel("Cantidad de samples <(n)>, índice <i>")
plt.ylabel("Distancias de grupos")
plt.show()

```

### Entrenamiento del modelo

Ahora que tenemos la cantidad de clusters seleccionada se procederá a entrenar el modelo. 

Entrenado el modelo en @fig-kmeans-1 se podrán notar las observaciones en un plano tridimensional donde el color representa el cluster al que pertenece la observación.

```{python}
#| label: fig-kmeans-1
#| fig-cap: Observaciones en 3 dimensiones de los clusters


from sklearn.cluster import KMeans


kmeans = KMeans(n_clusters=6,).fit(imputados_OneHot)
final=data.copy()
final["Clusters"]=kmeans.labels_


clustering= AgglomerativeClustering(n_clusters=5,linkage="ward")
clustering.fit(imputados_OneHot)
final["clusters"]=clustering.labels_


final.to_csv('base1.csv', index=False)

import plotly.express as px
import plotly.graph_objects as go

fig = px.scatter_3d(final, x="DEBT_MDN", y="PCTFLOAN", z="PCTPELL",color="Clusters",size_max=0.001)
fig.show()
```

## Analisis de resultados