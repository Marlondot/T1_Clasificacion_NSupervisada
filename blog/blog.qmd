---
title: "Segmentación de universidades en EEUU con énfasis en estudiantes de areas relacionadas a la computación"
format:
  html:
    code-fold: true
jupyter: python3
echo: false
theme:
          light: flatly
          dark: darkly
toc: true
---

## Introducción

Para realizar la segmentación utilizamos la base de datos encontrada en el link: https://data.world/exercises/cluster-analysis-exercise-2

## Importe y análisis de datos

A continuación procedemos a hacer el cargo de datos considerando aquellos suprimidos por privacidad como NaN: 

```{python}
#| label: tbl-import-presentacion-inicial
#| tbl-cap: Datos iniciales

import numpy as np
import pandas as pd
import seaborn as sb
from tabulate import tabulate 
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, ward

#TODO: Explicar valores Nan
#TODO: Revisar el tema de la low_memory=False
datos_raw = pd.read_csv('CollegeScorecard.csv', na_values='PrivacySuppressed',low_memory=False)

datos_raw.head(5)
```

Como se puede ver en @tbl-import-presentacion-inicial la base de datos contiene 7804 observaciones y 1725 columnas.

Para comenzar y como nuestro interés de acuerdo al objetivo *TODO: Esta justificación si es correcta* es tener la mayor cantidad de datos procederemos a descartar las columnas cuyo porcentaje de nulos sea mayor al (30%)

```{python}

def verificar_variables(nombre_archivo):
  path = f'/content/gdrive/Shareddrives/TAE/{nombre_archivo}'
  variable = pd.read_csv(path)
  nombre_ingresos = list(variable['variable_name'])
  df.loc[:, nombre_ingresos].info(verbose=True, show_counts=True)

def utiles_por_porcentaje(df, columnas, porcentaje):
  new_df = df[columnas]
  total = len(new_df)
  datos = {'columna': [], 'porcentaje_datos_nulos': [], 'datos_nulos': []}
  for col in new_df.columns:
    nulos = new_df[col].isna().sum()
    datos['columna'].append(col)
    datos['porcentaje_datos_nulos'].append(nulos/total)
    datos['datos_nulos'].append(f'{nulos} de {total}')
  nulos_columnas = pd.DataFrame(datos)
  mayor_02 = nulos_columnas['porcentaje_datos_nulos'] <= porcentaje
  utiles = nulos_columnas[mayor_02].sort_values(by='porcentaje_datos_nulos')
  return utiles

utiles_por_porcentaje(df=datos_raw, columnas=datos_raw.columns, porcentaje=0.3)

```

De acuerdo a un análisis manual y dado el objetivo planteado inicialmente se concluye que de las 351 variables resultantes las más adecuadas, en un principio, para ofrecer un producto son los presentados en @tbl-col-iniciales 

```{python}
#| label: tbl-col-iniciales
#| tbl-cap: Columnas iniciales a analizar

from IPython.display import Markdown
from tabulate import tabulate
#TODO: Solucionar type de datos
interes=['CURROPER', 'CONTROL', 'DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27', 'CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL', 'PCTPELL']
table = [[interes[0],datos_raw[interes[0]].dtype,"Explcación bien perrona"],
         [interes[1],datos_raw[interes[1]].dtype,"Explcación bien perrona"],
         [interes[2],datos_raw[interes[2]].dtype,"Explcación bien perrona"],
         [interes[3],datos_raw[interes[3]].dtype,"Explcación bien perrona"],
         [interes[4],datos_raw[interes[4]].dtype,"Explcación bien perrona"],
         [interes[5],datos_raw[interes[5]].dtype,"Explcación bien perrona"],
         [interes[6],datos_raw[interes[6]].dtype,"Explcación bien perrona"],
         [interes[7],datos_raw[interes[7]].dtype,"Explcación bien perrona"],
         [interes[8],datos_raw[interes[8]].dtype,"Explcación bien perrona"],
         [interes[9],datos_raw[interes[9]].dtype,"Explcación bien perrona"],
         [interes[10],datos_raw[interes[10]].dtype,"Explcación bien perrona"],
         [interes[11],datos_raw[interes[11]].dtype,"Explcación bien perrona"],
         [interes[12],datos_raw[interes[12]].dtype,"Explcación bien perrona"],
         [interes[13],datos_raw[interes[13]].dtype,"Explcación bien perrona"],
         ]
Markdown(tabulate(
  table, 
  headers=["Nombre","Tipo de dato", "Resumen"]
))
```

En cuanto al manejo de nulos resultantes como se puede ver en @tbl-col-iniciales-datos-perdidas la mayor perdida de datos se debe a las variables DEBT_MDN y GRAD_DEBT_MDN, que tienen entre un 14% y un 16% de datos nulos en comparación con los datos generales.


```{python}
#| label: tbl-col-iniciales-datos-perdidas
#| tbl-cap: Datos perdidos

columnas_categoricas=['CURROPER', 'CONTROL','CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL']
columnas_numericas=['DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27','PCTPELL']

col_interes_NaN_analisis_all=utiles_por_porcentaje(df=datos_raw, columnas=interes, porcentaje=0.3)

col_interes_NaN_analisis_interes=utiles_por_porcentaje(df=datos_raw[interes].dropna(subset=["PCTPELL"]), columnas=interes, porcentaje=0.3)

col_interes_NaN_analisis_interes

colA=list(col_interes_NaN_analisis_all["porcentaje_datos_nulos"])
colB=list(col_interes_NaN_analisis_interes["porcentaje_datos_nulos"])
table1=list()

for i in range(14):
  if interes[i] in columnas_categoricas:
    table1.append([interes[i],"Variable categorica",colA[i]])
  else:
    table1.append([interes[i],"Variable numerica",colA[i]])

"""Markdown(tabulate(
  table1, 
  headers=["Variable","Tipo de variable", "% NaN"]
))"""

utiles_por_porcentaje(datos_raw,interes,0.3)
```

Por lo que ahora procederemos con una imputación reemplazando con la mediana y la moda los valores nulos en columnas numéricas y categóricas respectivamente.

Luego de hacer la debida imputación podemos notar en @tbl-matriz-corr-1 que la deuda media acumulada de todos los estudiantes (DEBT_MDN) y la deuda media acumulada de los estudiantes que terminaron la carrera (GRAD_DEBT_MDN) presenta una correlación de 0.77, por lo que procedera a sacar una de ellas (GRAD_DEBT_MDN) para eliminar redundancia del conjunto de caracteristicas con las que se estan trabajando en el proceso de clusterización. También se logra observar unas correlaciones moderadas en otros pares de variables, son embargo se tomo la decisión de dejarlar, dado que son variables que contienen información inportante en el proceso de clusterización.


```{python}
#| label: tbl-matriz-corr-1
#| tbl-cap: Matriz de correlacion
#| 
datos_raw_interes=datos_raw[interes]
columnas_categoricas=['CURROPER', 'CONTROL','CIP11BACHL', 'CIP15BACHL',
 'CIP14BACHL', 'CIP27BACHL']
columnas_numericas=['DEBT_MDN', 'PCTFLOAN', 'GRAD_DEBT_MDN',
 'PCIP11', 'PCIP15', 'PCIP14', 'PCIP27','PCTPELL']

datos_interes_imputados=datos_raw_interes.copy()
datos_interes_imputados[columnas_numericas] = datos_raw_interes[columnas_numericas].fillna(datos_raw_interes[columnas_numericas].median())

for col in columnas_categoricas:
  datos_interes_imputados[col] = datos_raw_interes[col].fillna(datos_raw_interes[col].mode()[0])


corrMatrix=datos_interes_imputados.corr()
sb.heatmap(corrMatrix, annot=True)
plt.rcParams["figure.figsize"] = (20,20)
plt.show
```

